% -----------------------------------------------
% Template for ISMIR 2013
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{ismir2013,amsmath,cite}
\usepackage{graphicx}

% For Graphs
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{calc}
\usepackage{caption}

% Title.
% ------
\title{CSC 475 Final Project: Design Specification}

% Three addresses
% --------------
\threeauthors
{Lee Gauthier} {\tt lgauthie@uvic.ca}
{Robert Janzen} {\tt rjanzen@uvic.ca}
{Sondra Moyls} {\tt smoyls@uvic.ca}

\begin{document}
%
\maketitle
%

\section{Abstract}\label{sec:desoutline}
Our project aims to build an automatic chord recognition system using chroma
vectors and Hidden Markov Models. We will then compare the accuracy
of the models built using several different sets of features. The first model
will be built using chroma vectors extracted from unaltered audio files. The
second model will be built using chroma vectors extracted from audio files with
the transient elements removed. Time permitting, a third model will be created
using automatic voice extraction in place of chroma vectors.

\section{Data Set}

For this project we will use the annotated Billboard 100 data set. This data
set includes audio files for approximately 650 audio files from the Billboard
top 100 chart between the years 1958 and 1991. Each song also includes beat by
beat chord annotation\cite{Burgoyne:07}. The chords are labeled as either maj,
min, aug, dim, sus2, sus4, X, or N, where X and N imply no chord is present or detected.
For the scope of this project we will not try to classify inverted chords and
7th chords independently, and instead will classify them as one of the previously mentioned
classes.

\section{Pre-processing}
Audio will be pre processed by performing harmonic percussive source separation
(HPSS). It has been noted that applying this pre processing step can greatly
improve chord estimation accuracy\cite{Reed:09}.

In a spectrogram the harmonic components of a musical signal have a
stable pitch and form parallel ridges along the time axis. Transient sounds
distribute their frequency content across the entire spectrum and occur during
short time frames, which can be seen as spikes in the frequency axis. The end
product of HPSS is two signals, one with mostly harmonic content, the other
with percussive sounds which is obtained by complementary diffusion on the
spectrogram. The second stage of our project will use the harmonic content to
generate chroma vectors.

The constant-Q transformation can be used to achieve better frequency resolution 
in the low end and greater time resolution for high frequencies. This can be a useful 
step before performing median filtering during harmonic percussive source separation. 
Unfortunately the inverse transform does not allow for perfect reconstruction but the 
authors Klapuri and Schorkhuber have achieved a reasonable-quality inverse by using the 
conjugate transpose of the CQT kernel.

For our purposes, and with our coding expertise, it may be more feasible to accept 
some loss of separation accuracy by using a regular fourier transformation before the 
median filtering step. Fortunately both Python (scipy) and Matlab contain methods for 
fourier transforms and median filtering, which should allow us to start working with 
the HMM faster.

\section{Feature Extraction}

Chroma vectors will be generated in accordance to the time intervals given in
the billboard dataset.  We will experiement using the spectra2chroma and chromaFilter
MarSystems. Which one yields better results will be investigated as part of the
project.

The second feature we will potentially experiment with is using extracted
pitch contours instead of chroma vectors to train the models. Several voices could
be extracted from an audio recording, and then used as training data. More research needs
to be done to see if there are any models of voice extraction that would be feasible to
implement in the given time frame. Once the contours have been extracted the system will
check against the billboard data to find what chord is playing for each note of the melody.
This chord to pitch data would the be used to train the HMM.

% Define block style
\tikzstyle{block} = [rectangle, draw, fill=blue!20,
    text width=7em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']

\begin{figure}
\begin{tikzpicture}[node distance=3cm]
    % Place nodes
    \node [block] (init) {Billboard Data Set};
    \node [block, below left of= init] (chordlabel) {Labeled Chords By Beat (.txt)};
    \node [block, below right of= init] (audio) {Audio (.wav)};
    \node [block, below of= chordlabel] (chordname) {Chord Names at Beat Times};
    \node [block, below of= audio] (chroma) {12-bin chroma features};
    \coordinate (Middle) at ($(chordlabel)!0.5!(chroma)$);
    \node [block, below of  =Middle, yshift=-1cm] (hmm) {HMM};
    \node [below of  =Middle] (train) {Training};
    % Draw edges
    \path [line] (init) -- (audio);
    \path [line] (init) -- (chordlabel);
    \path [line] (chordlabel) -- node {Analyze Groud Truth} (chordname);
    \path [line] (audio) -- node {Analyze Chroma} (chroma);
    \path [line] (chroma) -- (hmm);
    \path [line] (chordname) -- (hmm);

\end{tikzpicture}
\caption{System Overview}
\end{figure}

\section{Hidden Markov Model}

Hidden Markov Models (HMMs) are currently the most common method for predicting
chord labels, and also produce high accuracy scores. Previous use of
preprocessing of input data has also been explored and is summarized in
\cite{McVicar:00}.

The output chroma vectors and the ground truth labels will be passed into the
hidden Markov model (HMM) function available through sci-kit, and will be used
to build a transition matrix. The HMM will have 74 states, one for each chord
of each pitch class, plus the two non-chord states X and N. We will investigate
further on the project how best to parse out training and testing data for the
model.

\section{Roles of Each Member}

Robert will work on the audio preprocessing (ie\. Transient removal).  Lee and
Sondra will work on the system for extracting chroma vectors at the correct
times, and feeding this data into the Hidden Markov Model. Time permitting a
voice based extraction will be fed into the same model.

\section{Proposed Timeline}\label{sec:timeline}

{\bf March 10-16}\newline
Rewrite design specification. Continue reading papers and solidify project
goals. Test out using Python bindings with Marsyas if haven't already. Start
working on preprocessing and manipulating the ground truth data to output
chroma vectors
\newline
{\bf March 17-23}\newline
Continue working on above.
\newline
{\bf March 24-30}\newline
Continue working on above. Start working towards writing progress report.
\newline
{\bf March 31-April 6}\newline
Write progress report. Start working on getting the HMM classifier working.
\newline
{\bf April 7 - April 13}\newline
Put everything together. Start outputting results for final paper/slides
\newline
{\bf April 14 - April 21}\newline
Make presentation slides. Finish any final testing.
\newline
{\bf Tuesday April 22}\newline
Report due and class presentation.

\section{Progress Report}\label{sec:progreport}


===
-- readings, changes in design
-- problems with python and changing frame size in marsyas
-- progess on HPSS
====

\begin{thebibliography}{citations}

\bibitem {McVicar:00}
M. McVicar, et al.
"Automatic Chord Estimation for Audio: A Review of the State of the Art''
{\it IEEE/ACM Transactions on Audio, Speech, and Language Processing},
Vol.~22,No.~2, pp.~1-20, 2014.

\bibitem{Ueda:01}
Yushi Ueda, et al.
"HMM-Based Approach For Automatic Chord Detection Using Refined Acoustic
Features''
{\it IEEE International Conference on Acoustics, Speech, and Signal Processing 2010},
pp.~5518--5521, 2010.

\bibitem{Varewyck:02}
Matthias Varewyck, et al.
"A Novel Chroma Representation of Polyphonic Music based on Multiple Pitch
Tracking Techniques''
{\it 16th ACM International Conference on Multimedia},
2008.

\bibitem{Lee:03}
Kyogu Lee
"Automatic Chord Recognition from Audio Using Enhanced Pitch Class Profile''
{\it Center for Computer Research in Music and Acoustics, Stanford},
2006.

\bibitem{Papadopoulus:04}
Hélène Papadopoulos and George Tzanetakis
"Modeling Chord and Key Structure With Markov Logic''
{\it 13th International Society for Music Information Retrieval Conference},
pp.~127-132, 2012.

\bibitem{Richardson:05}
Matthew Richardson and Pedro Domingos
"Markov Logic Networks,''
{\it Department of Computer Science and Engineering, University of Washington, Seattle, WA},
pp.~1-43, 2006.

\bibitem{SciKit:06}
F. Pedregosa, et al.
"Scikit-learn: Machine Learning in Python"
{\it Journal of Machine Learning Research},
Vol.~12,pp.~2825-2830, 2011.

\bibitem{Burgoyne:07}
John Ashley Burgoyne, Jonathan Wild, and Ichiro Fujinaga
"An Expert Gound-Truth Set For Audio Chord Recognition and Music Analysis,"
{\it 12th International Society for Music Information Retrieval Conference},
pp.~633-638, 2011.

\bibitem{Burgoyne:08}
Nanzhu Jiang et al.
"Analyzing Chroma Feature Types for Automated Chord Recognition,"
{\it AES 42nd International Conference },
pp.~1-10, 2011.

\bibitem{Reed:09}
J.T. Reed, Yushi Ueda, S. Siniscalchi, Yuki Uchiyama, Shigeki Sagayama, C.-H. Lee
"Minimum Classification Error Training To Improve Isolated Chord Recognition"
{\it 10th International Society for Music Information Retrieval Conference (ISMIR 2009)},
pp.~609-614, 2009.

\bibitem{Mauch:10}
Matthias Mauch
"Automatic Chord Transcription from Audio Using Computation Models of Musical Context"
{\it School of Electronic Engineering and Computer Science, Queen Mary, University of London},
pp.~1-168, 2010.

\bibitem{FitzGerald:11}
Derry FitzGerald
"Harmonic/Percussive Separation Using Median Filtering"
{\it Proc\. of the 13th Int. Conference on Digital Audio Effects (DAFx-10), Graz, Austria, September 6-10, 2010},
pp.~1-4, 2010.

\bibitem{Blunsom:12}
Phil Blunsom
"Hidden Markov Models"
{\it Melbourne School of Engineering},
pp.~1-7, 2004.

\bibitem{Sumi:13}
Kouhei Sumi, KatsutoshiItoyama, Kazuyoshi Yoshii, Kazunori Komatani, Tetsuya Ogata, and Hiroshi G. Okuno
"Automatic Chord Recognition Based on Probabilistic Integration of Chord Transition and Bass Pitch Estimation"
{\it ISMIR 2008 - Session 1a - Harmony},
pp.~39-44, 2008.

\bibitem{Ryyananen:14}
Matti P. Ryyananen and Anssi P. Klapuri
"Automatic Transcription of Melody, Bass Line, and Chords in Polyphonic Music"
{\it Computer Music Journal, Volume 32, Number 3},
pp.~72-86, Fall 2008.

\bibitem{Lee:15}
Kyogu Lee and Malcolm Stanley
"Automatic Chord Recognition from Audio Using an HMM with Supervised Learning"
{\it AMCMM '06},
pp.~10-11, 2006.

\bibitem{Papadopoulus:16}
Hélène Papadopoulos and Geoffroy Peeters
"Large-Scale Study of Chord Estimation Algorithms Based on Chroma Representation and HMM"
{\it CBMI 2007},
pp.~53-60, 2007.

\bibitem{Salamon:17}
Justin Salamon and Emilia G{\'o}mez
"Melody Extraction from Polyphonic Music Signals using Pitch Contour Characteristics"
{\it IEEE Transactions on Audio, Speech and Language Processing}
pp.~1759-1770, 2012.


\end{thebibliography}

\bibliography{ismir2013template}

\end{document}

